{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "class postgresql_db_config:\n",
    "    NAME = 'stock_data'\n",
    "    PORT = 5432\n",
    "    HOST = '162.246.156.44'\n",
    "    USER = 'stock_data_admin'\n",
    "    PASSWORD = 'ece493_team4_stock_data'\n",
    "\n",
    "    \n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    " \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def get_data(start,end,stock_name):\n",
    "    postgre_db = psycopg2.connect(dbname = postgresql_db_config.NAME,\n",
    "                                    user = postgresql_db_config.USER,\n",
    "                                    password = postgresql_db_config.PASSWORD,\n",
    "                                    host = postgresql_db_config.HOST,\n",
    "                                    port = postgresql_db_config.PORT)\n",
    "    sql =f'''\n",
    "    select * from public.stock_data_full where stock_name = '{stock_name}' order by time_stamp asc\n",
    "    '''\n",
    "    dat = sqlio.read_sql_query(sql, postgre_db)\n",
    "    dat = dat.dropna()\n",
    "    dat = dat.reset_index(drop=True)\n",
    "    print(f\"Now we are processing stock : {dat['stock_name'][0]}\")\n",
    "    features = dat[['open','volume','volume_obv','trend_macd','trend_macd_signal','trend_macd_diff','momentum_rsi','volume_vpt']]\n",
    "    dataset = features.values\n",
    "    data_mean = dataset.mean(axis=0)\n",
    "    data_std = dataset.std(axis=0)\n",
    "    dataset = (dataset-data_mean)/data_std\n",
    "    if end == None:\n",
    "        end = dataset.shape[0]\n",
    "    if start == None:\n",
    "        start = dataset.shape[0]-140\n",
    "    return dataset[start:end]\n",
    "\n",
    "\n",
    "def retrain_model(start,stock_name,TRAIN_SPLIT=None,end=None):\n",
    "    dataset = get_data(start,end,stock_name)\n",
    "    print(dataset.shape)\n",
    "    past_history = 70\n",
    "    future_target = 7\n",
    "    STEP = 1\n",
    "    x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 0], 0,\n",
    "                                                     TRAIN_SPLIT, past_history,\n",
    "                                                     future_target, STEP)\n",
    "    BATCH_SIZE = 30\n",
    "    BUFFER_SIZE = x_train_multi[0].shape[0]\n",
    "    train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "    train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "    multi_stap_model = tf.keras.models.load_model(f\"save_model_1400/{stock_name}.h5\")\n",
    "    multi_stap_model.fit(x_train_multi,y_train_multi,verbose=True,epochs = 15)\n",
    "    multi_stap_model.save(f\"saved_model/{stock_name}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are processing stock : aapl\n",
      "(354, 8)\n",
      "Train on 277 samples\n",
      "Epoch 1/15\n",
      "277/277 [==============================] - 3s 9ms/sample - loss: 0.2479\n",
      "Epoch 2/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.1784\n",
      "Epoch 3/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.1448\n",
      "Epoch 4/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.1204\n",
      "Epoch 5/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.1065\n",
      "Epoch 6/15\n",
      "277/277 [==============================] - 1s 3ms/sample - loss: 0.0977\n",
      "Epoch 7/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0931\n",
      "Epoch 8/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0880\n",
      "Epoch 9/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0836\n",
      "Epoch 10/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0792\n",
      "Epoch 11/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0768\n",
      "Epoch 12/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0750\n",
      "Epoch 13/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0747\n",
      "Epoch 14/15\n",
      "277/277 [==============================] - 1s 4ms/sample - loss: 0.0734\n",
      "Epoch 15/15\n",
      "277/277 [==============================] - 1s 3ms/sample - loss: 0.0709\n",
      "Now we are processing stock : v\n",
      "(353, 8)\n",
      "Train on 276 samples\n",
      "Epoch 1/15\n",
      "276/276 [==============================] - 3s 9ms/sample - loss: 0.4214\n",
      "Epoch 2/15\n",
      "276/276 [==============================] - 1s 3ms/sample - loss: 0.2682\n",
      "Epoch 3/15\n",
      "276/276 [==============================] - 1s 3ms/sample - loss: 0.2299\n",
      "Epoch 4/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1904\n",
      "Epoch 5/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1726\n",
      "Epoch 6/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1619\n",
      "Epoch 7/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1475\n",
      "Epoch 8/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1418\n",
      "Epoch 9/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1377\n",
      "Epoch 10/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1326\n",
      "Epoch 11/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1266\n",
      "Epoch 12/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1238\n",
      "Epoch 13/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1214\n",
      "Epoch 14/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1190\n",
      "Epoch 15/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1185\n",
      "Now we are processing stock : ge\n",
      "(353, 8)\n",
      "Train on 276 samples\n",
      "Epoch 1/15\n",
      "276/276 [==============================] - 3s 10ms/sample - loss: 0.4792\n",
      "Epoch 2/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2833\n",
      "Epoch 3/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1813\n",
      "Epoch 4/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1433\n",
      "Epoch 5/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1232\n",
      "Epoch 6/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1109\n",
      "Epoch 7/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1021\n",
      "Epoch 8/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0983\n",
      "Epoch 9/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0956\n",
      "Epoch 10/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0939\n",
      "Epoch 11/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0895\n",
      "Epoch 12/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0876\n",
      "Epoch 13/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0857\n",
      "Epoch 14/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0844\n",
      "Epoch 15/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0830\n",
      "Now we are processing stock : dis\n",
      "(352, 8)\n",
      "Train on 275 samples\n",
      "Epoch 1/15\n",
      "275/275 [==============================] - 3s 10ms/sample - loss: 1.4164\n",
      "Epoch 2/15\n",
      "275/275 [==============================] - 1s 3ms/sample - loss: 0.8514\n",
      "Epoch 3/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.3577\n",
      "Epoch 4/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.2150\n",
      "Epoch 5/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1850\n",
      "Epoch 6/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1595\n",
      "Epoch 7/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1485\n",
      "Epoch 8/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1334\n",
      "Epoch 9/15\n",
      "275/275 [==============================] - 1s 3ms/sample - loss: 0.1248\n",
      "Epoch 10/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1228\n",
      "Epoch 11/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1131\n",
      "Epoch 12/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1098\n",
      "Epoch 13/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1085\n",
      "Epoch 14/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1040\n",
      "Epoch 15/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1015\n",
      "Now we are processing stock : jpm\n",
      "(353, 8)\n",
      "Train on 276 samples\n",
      "Epoch 1/15\n",
      "276/276 [==============================] - 4s 16ms/sample - loss: 0.4819\n",
      "Epoch 2/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2365\n",
      "Epoch 3/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1800\n",
      "Epoch 4/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1398\n",
      "Epoch 5/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1256\n",
      "Epoch 6/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1148\n",
      "Epoch 7/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1076\n",
      "Epoch 8/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0992\n",
      "Epoch 9/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0956\n",
      "Epoch 10/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0916\n",
      "Epoch 11/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0895\n",
      "Epoch 12/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0855\n",
      "Epoch 13/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0818\n",
      "Epoch 14/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0806\n",
      "Epoch 15/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0778\n",
      "Now we are processing stock : wmt\n",
      "(352, 8)\n",
      "Train on 275 samples\n",
      "Epoch 1/15\n",
      "275/275 [==============================] - 3s 10ms/sample - loss: 0.3894\n",
      "Epoch 2/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.2602\n",
      "Epoch 3/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.2197\n",
      "Epoch 4/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1974\n",
      "Epoch 5/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1821\n",
      "Epoch 6/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1718\n",
      "Epoch 7/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1643\n",
      "Epoch 8/15\n",
      "275/275 [==============================] - 1s 3ms/sample - loss: 0.1585\n",
      "Epoch 9/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1539\n",
      "Epoch 10/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1491\n",
      "Epoch 11/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1452\n",
      "Epoch 12/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1409\n",
      "Epoch 13/15\n",
      "275/275 [==============================] - 1s 3ms/sample - loss: 0.1377\n",
      "Epoch 14/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1334\n",
      "Epoch 15/15\n",
      "275/275 [==============================] - 1s 4ms/sample - loss: 0.1303\n",
      "Now we are processing stock : intc\n",
      "(353, 8)\n",
      "Train on 276 samples\n",
      "Epoch 1/15\n",
      "276/276 [==============================] - 3s 9ms/sample - loss: 0.3265\n",
      "Epoch 2/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2234\n",
      "Epoch 3/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1848\n",
      "Epoch 4/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1563\n",
      "Epoch 5/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1436\n",
      "Epoch 6/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1332\n",
      "Epoch 7/15\n",
      "276/276 [==============================] - 1s 3ms/sample - loss: 0.1261\n",
      "Epoch 8/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1227\n",
      "Epoch 9/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1180\n",
      "Epoch 10/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1136\n",
      "Epoch 11/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1112\n",
      "Epoch 12/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1092\n",
      "Epoch 13/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1095\n",
      "Epoch 14/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1057\n",
      "Epoch 15/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1034\n",
      "Now we are processing stock : msft\n",
      "(353, 8)\n",
      "Train on 276 samples\n",
      "Epoch 1/15\n",
      "276/276 [==============================] - 3s 10ms/sample - loss: 0.2850\n",
      "Epoch 2/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2057\n",
      "Epoch 3/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1663\n",
      "Epoch 4/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1464\n",
      "Epoch 5/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1311\n",
      "Epoch 6/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1242\n",
      "Epoch 7/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1178\n",
      "Epoch 8/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1141\n",
      "Epoch 9/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1089\n",
      "Epoch 10/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1051\n",
      "Epoch 11/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1005\n",
      "Epoch 12/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0978\n",
      "Epoch 13/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0940\n",
      "Epoch 14/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0921\n",
      "Epoch 15/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0901\n",
      "Now we are processing stock : nke\n",
      "(353, 8)\n",
      "Train on 276 samples\n",
      "Epoch 1/15\n",
      "276/276 [==============================] - 3s 9ms/sample - loss: 0.5529\n",
      "Epoch 2/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.3619\n",
      "Epoch 3/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2628\n",
      "Epoch 4/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2256\n",
      "Epoch 5/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1975\n",
      "Epoch 6/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1750\n",
      "Epoch 7/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1650\n",
      "Epoch 8/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1569\n",
      "Epoch 9/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1490\n",
      "Epoch 10/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1416\n",
      "Epoch 11/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1351\n",
      "Epoch 12/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1309\n",
      "Epoch 13/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1275\n",
      "Epoch 14/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1232\n",
      "Epoch 15/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1203\n",
      "Now we are processing stock : ibm\n",
      "(353, 8)\n",
      "Train on 276 samples\n",
      "Epoch 1/15\n",
      "276/276 [==============================] - 3s 9ms/sample - loss: 1.0801\n",
      "Epoch 2/15\n",
      "276/276 [==============================] - 1s 3ms/sample - loss: 0.4171\n",
      "Epoch 3/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2585\n",
      "Epoch 4/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.2006\n",
      "Epoch 5/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1746\n",
      "Epoch 6/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1478\n",
      "Epoch 7/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1327\n",
      "Epoch 8/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1199\n",
      "Epoch 9/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1144\n",
      "Epoch 10/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1107\n",
      "Epoch 11/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.1038\n",
      "Epoch 12/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0986\n",
      "Epoch 13/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0953\n",
      "Epoch 14/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0943\n",
      "Epoch 15/15\n",
      "276/276 [==============================] - 1s 4ms/sample - loss: 0.0951\n"
     ]
    }
   ],
   "source": [
    "for i in dat['stock_name']:\n",
    "    retrain_model(start = 1400,end= None,stock_name = i,TRAIN_SPLIT=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    aapl\n",
       "1       v\n",
       "2      ge\n",
       "3     dis\n",
       "4     jpm\n",
       "5     wmt\n",
       "6    intc\n",
       "7    msft\n",
       "8     nke\n",
       "9     ibm\n",
       "Name: stock_name, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
